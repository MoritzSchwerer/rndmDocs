#+TITLE: Von Effectivitaet zu Effizienz durch Approximation
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-bigblow.setup


Wenn man im Duden die Wörter Effizienz und Effektivität sucht, wird man zu dem Ergebnis verleitet, dass die beiden Wörter wahllos
für einander ausgetauscht werden können. Die Realität ist jedoch eine andere.
So ist beispielsweise das lineare iterieren eines 1-dimensionalen in aufsteigender Wertigkeit sortierten Sets "S" auf der Suche nach dem
Element "e" zwar ein durchaus effektiver Prozess, denn jedes erdenkliche Element "e", dass sich in dem Set "S" befindet, kann
in durchschnittlich n/2 Iterationen gefunden werden, wobei "n" die Anzahl der Elemente in "S" darstellt. In der Ohmschen Komplexitätsanalyse wird die Komplexität dieses Verfahrens als O(N) oder linear bezeichnet. Eine lineare Komplexität mag in sich selbst nicht sehr aufwändig wirken, wenn man
allerdings die moderne Informationstechnologie betrachtet sind Listen/Arrays (für unsere Zwecke austauschbar durch Sets) mit Größen von über
10 Millionen Elementen keine Seltenheit, wodurch sich zeigt, dass Effektivität alleine nicht die richtige Lösung für moderne Probleme darstellt.

Wir brachen Effizienz!
Aber was heißt es effizient zu sein?
Um effizient zu sein, muss man auch effektiv sein, andersherum ist das jedoch nicht der Fall.
Hier ist wohl möglich der Punkt, in dem die Verwirrung der Schreiber des Dudens ihren Ursprung hat.
Um zurück zu dem oben genannten Beispiel der Suche eines Element "e" in einem Set "S" zu kommen versuchen wir
die lineare Komplexität zu verbessern, wobei unser Maß der Effizienz die Anzahl an atomaren Prozessen in unserer Suche beträgt.
Um dies zu erzielen, können unterschiedliche Algorithmen aus der Mathematik in Erwägung gezogen werden. Darunter das "Iterative Näherungsverfahren", und die"Interpolations-Suche". Einfachheitshalber beschäftigen wir uns mit dem Ersten.
Das Vorgehen ist wie folgt: Suche den mittleren Index des zu iterierenden Mediums, vergleiche dessen Wert, mit dem der gesucht ist, haben wird das
richtige Element gefunden sind wir fertig, ist das gefundene Element zu groß,  beschränken wir unsere Suche auf die kleinere Hälfte, und dementsprechend das Gegenteil, wenn wir einen kleineren Wert vorfinden. Der erprobte Mathematiker vermag hier möglicherweise das "Divide and Conquer"
Verfahren zu erkennen, was darauf hinweist, dass es sich um einen logarithmischen Algorithmus handelt.
Und durch Induktion lässt sich auch genau das beweisen. Die Komplexität beträgt nämlich O(log(N)), was bedeutet, wenn ein linearer Algorithmus
1.000.000.000 atomare Aktionen verarbeiten müsse, so muss unser logarithmisches Verfahren gerade mal 30 (log2(1.000.000.000)) atomare
Aktionen durchführen, wobei für jeden Faktor 1000 der Elemente bei dem logarithmischen Verfahren nicht Faktor 1000-mal mehr Arbeit geleistet
werden muss, sondern nur Faktor 10-mal mehr.
Aber was bedeutet das? Ein effektiver Algorithmus funktioniert, er erledigt eine Aufgabe. Aber er erledigt diese Aufgabe nicht unbedingt
Effizient. Effektivität bedeutet eine Aufgabe abschließen zu können, wobei Effizienz bedeutet eine Aufgabe so gut wie möglich abzuschließen.
"So gut wie möglich" kann hierbei auf die Dauer, die Kosten oder auch physische oder psychische Erschöpfungen bezogen sein.

Aber was für eine Rolle spiel die Approximation in dem Ganzen?
Wie der Titel zu vermuten anregt, ist die Approximation ein Werkzeug, um von Effektivität zu Effizienz zu gelangen.
So gibt es unterschiedliche Funktionen in der Mathematik und der Physik, die schwer zu berechnen sind. Unter anderem cos, sin, tan aber auch etwas
wie die inverse einer Wurzel. Verschiedene Approximierungsverfahren bieten die Möglichkeit, etwas von der Genauigkeit für Effizienz zu opfern.
Die Kunst der Approximierung ist es, so viele unterliegende Attribute wie möglich von einer Funktion "F" zu erhalten, während ihre Komplexität
jedoch relevant verringert wird. Ein bekanntes Beispiel ist der Quake III Algorithmus, welche das Newton Verfahren anwendet, um die sehr aufwändige
und sehr häufige Berechnung der inversen Quadratwurzel von x zu berechnen. Diese Kalkulation beinhaltet gleich 2 sehr aufwändige Prozesse, die
Division und das Wurzelziehen. Die genaue Funktionalität ist irrelevant. Ein wichtiges Highlight jedoch ist die iterative Näherung an ein
lokales Optimum, was durch das Ableiten und dann eine probabilistische Rechnung der idealen Verschiebung erzielt wird. Dieser Prozess kann für
eine endliche Anzahl an Iterationen durchgeführt werden oder bis die Verbesserung einen gegebenen Wert unterschreitet.

Wie relevant sind diese Prozesse der iterativen Approximation jedoch für uns?
Grundlegend kann man sagen, dass momentan jeder Algorithmus in der Künstlichen Intelligenz eine Art der interaktiven Approximation verwendet,
genauer gesagt nennt sich der hier angewandte Algorithmus "Gradient Descent" der nur ein paar zusätzliche Parameter gegenüber dem Newton Verfahren
hat. Des Weiteren wird in quasi jeder Form von Telekommunikation eine Art der Funktionsapproximierung angewendet, die den Namen "Fourier Transform"
trägt, wobei hier ein Wellenbild mit einer einfach zu berechnenden Funktion (im Normalfall ein Polynom) approximiert wird, um möglichst
wenige Daten übertragen zu müssen - oder um auf den Titel dieses Artikels zurückzukommen, die Datenübertragung effizienter macht.
